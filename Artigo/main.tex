\documentclass{article}

\usepackage{amsmath}
\usepackage{PRIMEarxiv}
\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage[round,authoryear]{natbib}
\graphicspath{{img/}}     % organize your images and other figures under media/ folder

% \renewcommand{\tablename}{Tabela}
% \renewcommand{\figurename}{Figura}
% \renewcommand{\abstractname}{Resumo}
% \renewcommand{\keywords}[1]{
%   \textbf{Palavras-chave:} #1
% }
% \renewcommand{\keywords}[1]{%
%   \vspace{0.5em}
%   {\small\textbf{Palavras-chave:} #1}
% }

% ==== AJUSTAR ABSTRACT E KEYWORDS PARA PORTUGUÊS ====
\makeatletter

\renewcommand{\abstractname}{Resumo}

\renewenvironment{abstract}{%
  \par\vspace{1ex}%
  \begin{center}
    \bfseries \abstractname
  \end{center}
  \smallskip
}{%
  \par\vspace{1ex}%
}

\renewcommand{\keywords}[1]{%
  \par\vspace{1ex}%
  \noindent\textbf{Palavras-chave:}~#1%
}

\makeatother

% ==========================================
% LTeX: language=pt-BR

%% Title
\title{Simulador de Batalhas Pokémon}

\author{
  Eduardo Dalmás Faé, João Pedro Kuhn Braun \\
  Instituto de Informática \\
  Universidade Federal do Rio Grande do Sul \\
  Porto Alegre\\
  \texttt{\{edfae, jpkbraun\}@inf.ufrgs.br} \\
}


\begin{document}
\maketitle

\begin{abstract}
Esta pesquisa aborda uma simulação simplificada de batalhas Pokémon e o treinamento de agentes por aprendizado por reforço para a validação deste ambiente utilizando os seguintes algoritmos: A2C, DQN, PPO e RPPO, sendo esses testados em combates entre si para medição do desempenho alcançado. Os agentes foram treinados utilizando uma função de recompensa simples e uma mais elaborada, integrando recompensas a curto prazo. Eles foram divididos em grupos e colocados para disputar combates entre si, e foi observado, com exceção do A2C, que a função de recompensa mais elaborada capturou melhor as características de uma batalha, acelerando a convergência desses agentes. Foi observado também que os agentes aprenderam as regras do ambiente simplificado de batalha, que possui diversas possíveis melhorias a serem implementadas rumo a sua versão não simplificada, resultando em futura pesquisa.
\end{abstract}

% keywords can be removed
\keywords{Aprendizado por Reforço; Batalha Pokémon; Ambiente Simplificado}


\section{Introdução}

Pokémon é uma franquia da Nintendo que começou como um jogo no ano de 1996, e com o passar dos anos evoluiu para muitas outras categorias de conteúdo, tornando-se mundialmente reconhecida. Entre muitas das dinâmicas acerca do jogo, que trata desde narrativa de histórias até explorações de cenários e segredos, pode-se afirmar que uma das mecânicas centrais que traz vida e emoção ao jogo são as batalhas Pokémon. 

O mundo de Pokémon é composto por uma grande seleção de monstros, denominados Pokémon, que possuem suas próprias espécies, cadeias de evolução e características. Nesse mundo, surge uma nova profissão, o treinador Pokémon, cujo objetivo é capturar, treinar, cuidar e batalhar com os diferentes Pokémon espalhados por cada região. Em uma batalha Pokémon, dois treinadores se juntam para batalhar até que todos os Pokémon de um sejam derrotados, declarando assim o treinador com Pokémon ainda vivos como vencedor.

Após se tornar uma sensação global, Pokémon ganhou uma grande legião de fãs que buscam se desafiar das mais diversas maneiras. Enquanto alguns introduzem desafios para os jogos base, outros buscam aprimorar suas habilidades batalhando com outras pessoas, o que culminou em simuladores como o \href{https://Pokémonhowdown.com/}{Pokémon Showdown}, além de diversos torneios que ocorrem regularmente ao redor do mundo. Esse grande foco nas batalhas, especialmente no PVP (\textit{player versus player}), como já acontece com diversos jogos, instiga a criação de modelos capazes de travar batalhas. 

Enquanto a franquia Pokémon já traz IAs para os adversários desde o primeiro jogo lançado, as IAs presentes nos jogos antigos eram baseadas em regras e condições, o que permitia que fossem facilmente exploradas pelos jogadores. Essas fragilidades levantam o questionamento se técnicas mais robustas de RL não gerariam modelos com melhor desempenho nas batalhas.

Em prol de permitir o treinamento facilitado dos modelos, este trabalho objetiva desenvolver um ambiente simples e padronizado que permita a futuros pesquisadores e interessados treinar seus modelos de forma prática e eficiente. Para isso, utilizaremos uma abordagem incremental que busca partir de uma representação simples do ambiente da batalha Pokémon e ir progressivamente adicionando novas \emph{features}, aumentando aos poucos a complexidade. Além disso, faremos uso de bibliotecas focadas especificamente no treinamento de modelos para tarefas de RL, e testaremos duas estratégias diferentes de recompensa.

\section{Conceitos Básicos}

Desde 1959, ano em que se cunhou o termo \textit{machine learning} (aprendizado de máquina) devido à pesquisa de \citet{machinelearning}, na qual um agente foi treinado e obteve excelente desempenho no jogo de damas, muita pesquisa foi realizada nessa área e em aprendizado por reforço. Entre muitos marcos nesses campos de conhecimento, podemos citar Deep Blue \citep{deepblue}, IA que foi capaz de vencer o campeão de xadrez mundial da época, Garry Kasparov. Mais tarde, AlphaGo \citep{alphago} e AlphaZero \citep{alphazero} vieram para derrotar o campeão mundial de Go, Lee Sedol, e propor uma técnica para treinar um agente para jogar com desempenho sobrehumano diversos jogos de tabuleiro, sem necessidade de aprendizado supervisionado ou anotações de especialistas para aprender boas jogadas, respectivamente.

Indo além de jogos que podem ser representados por um domínio discreto, também houve grandes contribuições, como o DQN para jogar Atari \citep{dqn}, AlphaStar \citep{alphastar} e OpenAI-5 \citep{openai5}. DQN (\textit{Deep-Q Network}) é muito semelhante a \textit{Q-learning} tabular, porém no lugar da tabela de transições para cada par de ação e estado, uma rede neural se torna a responsável por aprender essa representação e, a partir de um estado, decidir a ação que deve ser tomada. Ela foi utilizada pela primeira vez para jogar diversos jogos do Atari, em 2013, resultando em uma publicação mais elaborada em 2015. AlphaStar e OpenAI-5 também tratam de jogos representados por um domínio contínuo, sendo o primeiro responsável por alcançar domínio altamente competitivo em StarCraft II e o segundo por vencer dos campões mundiais do mundial de Dota 2 no ano de 2019.

Sabemos que as aplicações de aprendizado por reforço estão distantes de estarem restritas a jogos. Após descobrirmos o que é possível no universo de jogos e simulações, abstraímos esse conhecimento e diversas áreas podem usufruir dele, como a robótica, a qual se beneficia muito de treinos de agentes em simulações para futura aplicação no mundo real, e como no treinamento de grandes modelos de linguagem \citep{hfrl}, em que através do \textit{feedback} humano é possível melhorar a qualidade das respostas desses modelos por via de aprendizado por reforço, entre muitas outras. Isso demonstra a importância adquirida por RL nas mais diversas áreas, e como modelos desenvolvidos para abordagens específicas podem se tornar úteis em situações completamente desconexas de seu propósito inicial. Assim, evidencia-se como não existe no âmbito de RL uma tarefa inútil, afinal, mesmo tarefas extremamente simples e desconexas da realidade podem ser utilizadas para treinar modelos que posteriormente serão aplicados em áreas completamente distintas.

\subsection{Batalha Pokémon}
Pokémon são seres vivos que se assemelham com animais, e os treinadores desses Pokémon os capturam e usam para batalhar entre si. Todo Pokémon possui atributos próprios, uma tipagem, composta de um ou dois tipos, de um a quatro ataques, cada um possuindo um tipo, nível de poder e podendo ter um efeito especial, dentre outros. A batalha ocorre de maneira síncrona, em turnos em que os jogadores decidem suas ações simultaneamente. Cada treinador Pokémon decide se deseja trocar o Pokémon atual para outro de sua equipe, se possível, ou atacar com um de seus movimentos válidos. Caso um único treinador decida trocar, a troca ocorre primeiro, caso ambos troquem, a ordem não importa, e se ambos Pokémon atacarem, executa sua ação primeiro o mais veloz.

Os tipos do Pokémon são importantes de serem levados em consideração, principalmente no contexto da batalha. No mundo Pokémon existem 18 tipos, cada um possuindo características positivas, negativas ou neutras em relação aos outros, como duplicar, cortar pela metade ou anular o dano causado pelo ataque de um oponente. Além disso, quando um Pokémon ataca utilizando um movimento categorizado com seu próprio tipo, o poder deste golpe é aumentado em um fator de 50\%, o que é chamado de \emph{Same-Type Attack Bonus} (STAB).

A ação durante uma batalha pode ser trocar o Pokémon que atualmente está em campo por um dos cinco que estão em descanso (totalizando uma equipe de seis) ou utilizar um de seus quatro movimentos em batalha contra seu oponente, resultando em um espaço de nove ações. O espaço de estados de uma batalha consiste em todas as informações dos Pokémon do próprio jogador e tudo que o oponente já revelou, como Pokémon já utilizados em campo pelo adversário, movimentos já usados, vidas atuais, efeitos especiais de habilidades ou itens, dentre outros.

Quando se tratando de RL no âmbito da batalha Pokémon, um dos desafios, que também se faz presente em diversos outros jogos, é a adversatividade, isso é, dois ou mais agentes competem entre si durante o jogo. Isso traz inúmeras complicações para o treinamento, dificultando que um modelo aprenda completamente sozinho. Para evitar esse problema, muitas técnicas já foram desenvolvidas, como o \textit{self-play}, onde um modelo joga contra uma cópia de si, o que pode ser feito utilizando versões antigas congeladas do modelo, como visto no AlphaGo \citep{alphago}.

Outro desafio é a simultaneidade, onde o estado da batalha apenas progride quando ambos agentes envolvidos escolhem seus movimentos. Dessa forma, como os turnos de cada agente acontecem ao mesmo tempo, torna-se necessário um tratamento para evitar vazamento de informações entre os modelos, para que nenhum dos jogadores possua uma vantagem inerente de conhecer previamente a ação escolhida por seu oponente.

\section{Trabalhos Relacionados}

\textbf{Batalha Pokémon} Trabalhos anteriores já modelaram ambientes de batalha Pokémon para treino e teste de agentes. \citet{poke-battle} definiram um ambiente simplificado de Pokémon para iniciar seus experimentos, reduzindo o espaço de estados e de ações possíveis. Para o treino de seus agentes, foram utilizados dois algoritmos, WPL (\textit{Weighted Policy Learner}) \citep{wpl} e GIGAWoLF (\textit{Generalized Infinitesimal Gradient Ascent Win or Learn Fast}) \citep{gigawolf}. Depois, foram então testados em um ambiente determinístico criado pelos próprios autores, em que o jogador 1, selecionando a estratégia correta durante o jogo, sempre deveria vencer, para descobrir que os agentes aprenderam a executar uma ação ruim a curto prazo, trocar de Pokémon, para uma possível vitória num momento posterior. O presente trabalho utilizou de um ambiente mais complexo, além de definir mais agentes, com diferentes algoritmos para seus treinamentos.

\textbf{Padronização de Batalhas} Houve tentativas de padronizar o cenário geral de batalha Pokémon, além de permitir a integração de agentes treinados com simuladores de batalhas geralmente utilizados hoje em dia, como \href{https://Pokémonhowdown.com/}{Pokémon Showdown}. Entretanto, trabalhos como os de \citet{poke-env} falham hoje em executar esse proposta com êxito, visto alguns problemas de uso e falta de documentação adequada. Esta pesquisa não integra o agente treinado com simuladores para interação e teste com outros jogadores, porém traz uma padronização não só de um ambiente de batalha Pokémon através do uso do \href{https://pettingzoo.farama.org/}{PettingZoo} \citep{pettingzoo}, mas também traz padronização no treino e uso de agentes no ambiente proposto.

\textbf{Montagem de Equipe} Uma etapa anterior ao combate em batalhas é a montagem de uma equipe adequada de Pokémon. \citet{vgc-ai} acompanham o \textit{meta game} atual do simulador de batalha, Pokémon Showdown, para restringir as possibilidades de diferentes opções para criar equipes, focando em estratégias voltadas para o jogo competitivo quando nessa criação. Este trabalho, no intuito de generalizar para o agente poder batalhar em toda e qualquer situação em que ele se encontrar, foca em um \textit{meta game} parcial ao gerar os times dos agentes, mas ainda mantém uma aleatoriedade forte, propiciando generalização para os mais diferentes cenários.


\section{Metodologia}

Inicialmente, buscou-se desfazer gradualmente as simplificações no ambiente de batalha Pokémon realizadas por \citet{poke-battle}, usando seu trabalho de base para o início da pesquisa. Reintroduzimos tipos duplos para os Pokémon, que agora podem representar toda a tipagem encontrada nesse universo, e criamos uma nova configuração para o ambiente de batalha Pokémon, denominada \textit{meta aware}. Tal configuração é semi-determinística e reflete uma das possíveis estratégias usadas por jogadores de alto nível em batalhas Pokémon, que consiste em escolher movimentos da tipagem do próprio Pokémon, a fim de se beneficiar de STAB, enquanto os demais movimentos são o que chamamos de movimentos de cobertura. Dado um Pokémon do tipo X com fraqueza Y, seleciona-se um golpe do tipo Z, o qual é forte contra o tipo Y, seguindo o raciocínio de que pode ser útil ter um golpe poderoso contra o tipo de um inimigo poderoso contra mim, a fim de poder revidar os ataques do oponente Y, como exemplificado na Figura \ref{fig:movimento-cobertura.png}.

\begin{figure}[h]
    \centering
    \caption{Exemplo da estratégia de movimentos de cobertura. No grafo direcionado abaixo, uma aresta que sai de um Pokémon (P) aponta para os movimentos (M ou Mov) que ele possui, enquanto uma aresta que sai de um movimento aponta para um Pokémon frágil contra o tipo do movimento. Logo, observando, é evidente que um Pokémon do tipo fogo é frágil contra água, então, a estratégia de nossa configuração \textit{meta aware} traz, como um possível movimento de cobertura, o tipo fogo ter um ataque elétrico, que é forte contra água, para poder se defender de adversários fortes contra ele. ``P'' é usado como abreviação de Pokémon, ``M'' de movimento, e todos movimentos numerados não foram especificados.}
    \includegraphics[width=0.7\textwidth]{img/movimento-cobertura.png}
    \label{fig:movimento-cobertura.png}
\end{figure}

O código de base é originário de 2020, e foi criado utilizando a versão 1.x do \href{www.tensorflow.org/}{TensorFlow} para o treinamento de seus agentes, com os algoritmos WPL \citep{wpl} e GIGAWoLF \citep{gigawolf}. Após múltiplas tentativas de adaptação do código para versões recentes do TensorFlow, e de execução em retrocompatibilidade, optamos por reformular o trabalho a fim de permitir uma maior compatibilidade com bibliotecas atuais. Dessa forma, utilizamos a biblioteca de Python PettingZoo \citep{pettingzoo} para o desenvolvimento do ambiente, pela sua padronização específica para treinamento multiagente, bem como a biblioteca \href{stable-baselines3.readthedocs.io}{Stable-Baselines3} \citep{sb3} para a escolha dos modelos usados, pela reprodutibilidade e grande gama de agentes.

Treinamos quatro agentes com algoritmos diferentes, sendo estes PPO (\textit{Proximal Policy Optimization}) \citep{ppo}, DQN (\textit{Deep-Q Networks}) \citep{dqn}, RPPO (\textit{Recurrent} PPO), uma versão do PPO que utiliza de LSTMs (\textit{Long Short-Term Memory}) \citep{lstm} para implementar recorrência, e A2C, uma versão de algoritmo \textit{actor-critic} da implementação de \citet{a3c} que, na implementação dentro do Stable-Baselines3, não é assíncrono que nem a versão A3C. Todos eles treinaram contra agentes aleatórios, que com mesma probabilidade, escolheriam qualquer opção do espaço de ações. A escolha desses algoritmos para o treinamento dos agentes foi devido à sua importância e bom desempenho na literatura, tendo cada um treinado por 1000000 de interações contra o agente aleatório no ambiente.

Além disso, no ambiente simplificado de Pokémon original de \citet{poke-battle}, foi feita uma engenharia de recompensa, mudando a dinâmica óbvia, uma recompensa de mais um quando o agente obtém uma vitória e menos um quando derrotado, para uma equação que leva em consideração alguns retornos imediatos, sendo estes: o dano causado e recebido por seus Pokémon e um retorno positivo quando um Pokémon adversário é derrotado. Segue, na Equação \eqref{eq:old-reward}, a descrição matemática desse cálculo, proposto pelo trabalho original, para a função de recompensa. Observa-se que o retorno para o agente tende a ser mais positivo do que negativo, o que pode ocasionar recompensas positivas mesmo em cenários de derrota, como quando um agente X derrota um Pokémon do adversário e deixa o segundo com metade de seus pontos de vida, de forma que, mesmo perdendo a partida, ainda receberá +0,5 pontos de recompensa.

\begin{equation}
    r_t
    = \sum_{i=1}^{2} \left( \Delta h^{\text{enemy}}_{i,t} + \Delta k^{\text{enemy}}_{i,t} \right)
      - \sum_{j=1}^{2} \Delta h^{\text{ally}}_{j,t},
    \qquad -2 \le r_t \le 4
    \label{eq:old-reward}
\end{equation}

Também houve a necessidade de adicionar um encerramento precoce nos episódios de combate Pokémon: o empate. Numa batalha tradicional do jogo não simplificado, não há o empate, pois em algum momento ela vai ter um fim e algum jogador terá a vitória. No entanto, foi observado durante o treinamento e teste dos agentes que, durante o combate, duas situações inconvenientes podem ocorrer: ambos os jogadores decidem que a melhor ação é trocar o seu Pokémon para sempre e, como esse movimento válido não causa dano, a batalha não se encerra, ou ambos não obtiveram conhecimento o suficiente para generalizar para todos os cenários, não aprendendo que alguns movimentos são completamente anulados contra alguns tipos, fazendo com que ninguém cause dano, tornando o jogo infinito. Devido a essas exceções, foi necessária a implementação do empate, que ocorre quando ambos os agentes, num combate, executarem consecutivamente quatro ações que não causem dano nenhum em seu oponente, sendo esta mecânica de empate somente implementada para os agentes treinados com a fórmula simples de recompensa, ocasionando menos um de retorno para ambos os agentes, análogo a como se ambos fossem derrotados.

Visto essa situação, optamos por treinar os quatro agentes na configuração do ambiente de batalha como \textit{meta aware}, em prol de permitir que experienciem cenários variados, porém análogos a uma batalha real. Cada um dos quatro foi treinado tanto com a função de recompensa original quanto com a função de recompensa simplificada, um ponto para a vitória, menos um para a derrota e empate. Assim, foram treinados oito agentes, que, para a avaliação de seus desempenhos, foram colocados para batalhar entre si em grupos de quatro divididos pela função de recompensa utilizada para treiná-los. Após isso, cada algoritmo lutou contra sua versão de recompensa alternativa, na tentativa de medir se o agente com a recompensa simples ou a original teria um melhor desempenho. Para cada teste, foram utilizadas duas configurações de ambiente, a nossa (\textit{meta aware}) e uma configuração determinística, em que o jogador um tem uma grande vantagem contra seu oponente, caso realize a troca como sua primeira ação, que se corretamente aproveitada deve garantir uma taxa de vitória de 100\%. Para todos os jogos, os agentes ocuparam a posição de jogador um e também de jogador dois, o que não deve impactar jogos na configuração \textit{meta aware}, mas possui grande impacto para a configuração determinística, onde o jogador um possui vantagem.

\section{Resultados e Discussão}

Foram realizados 16 jogos onde as versões com recompensa simples enfrentaram suas versões com recompensa mais elaborada, além de 24 jogos para cada competição interna entre modelos, dado o tipo de recompensa utilizada no treinamento, totalizando assim 64 jogos. Em todas as tabelas, há o sufixo ``-e'', indicando que foi utilizada a função elaborada por engenharia de recompensa encontrada pelos autores de Pokémon Battle Simulator \citep{poke-battle}, enquanto ``-s'' indica a função de recompensa simples, de mais um para vitória e menos um para derrotas ou empates. Para cada jogo citado, foram realizados 10000 batalhas de teste.

\begin{table}[ht]
    \centering
    \caption{Resultados dos jogos com a função de recompensa elaborada, no ambiente determinístico. ``V'', ``E'' e ``D'' são o número de vitórias, empates e derrotas que o jogador 1 obteve, respectivamente.}
    \label{tab:old_full_deterministic}
    \begin{tabular}{llrrr}
        \hline
        Jogador 1 & Jogador 2 & V & E & D \\
        \hline
        A2C-e & DQN-e & 7560 & \textbf{0} & 2440 \\
        A2C-e & PPO-e & 7509 & \textbf{0} & 2491 \\
        A2C-e & RPPO-e & 7549 & \textbf{0} & 2451 \\
        DQN-e & A2C-e & \textbf{10000} & \textbf{0} & 0 \\
        DQN-e & PPO-e & \textbf{10000} & \textbf{0} & 0 \\
        DQN-e & RPPO-e & \textbf{10000} & \textbf{0} & 0 \\
        PPO-e & A2C-e & \textbf{10000} & \textbf{0} & 0 \\
        PPO-e & DQN-e & \textbf{10000} & \textbf{0} & 0 \\
        PPO-e & RPPO-e & \textbf{10000} & \textbf{0} & 0 \\
        RPPO-e & A2C-e & 7574 & \textbf{0} & 2426 \\
        RPPO-e & DQN-e & 7548 & \textbf{0} & 2452 \\
        RPPO-e & PPO-e & 7455 & \textbf{0} & \textbf{2545} \\
        \hline
    \end{tabular}
\end{table}

Podemos observar que, na Tabela \ref{tab:old_full_deterministic} os agentes DQN e PPO foram capazes de generalizar, mesmo partindo de um treinamento em uma configuração diferente (pois todos agentes foram treinados somente na configuração \textit{meta aware}), para todos os jogos determinísticos. A vantagem entregue ao jogador um garante a sua vitória caso jogue adequadamente. Sendo assim, dois agentes falharam na generalização para esse cenário, sendo estes A2C e RPPO. Apesar disso, é interessante que não houve nenhum empate, todos os caminhos levaram aos encerramentos das partidas.

\begin{table}[ht]
    \centering
    \caption{Resultados dos jogos com a função de recompensa elaborada, no ambiente \textit{meta aware}. ``V'', ``E'' e ``D'' são o número de vitórias, empates e derrotas que o jogador 1 obteve, respectivamente.}
    \label{tab:old_meta_game_aware}
    \begin{tabular}{llrrr}
        \hline
        Jogador 1 & Jogador 2 & V & E & D \\
        \hline
        A2C-e & DQN-e & 5231 & 99 & 4670 \\
        A2C-e & PPO-e & 4986 & 116 & 4898 \\
        A2C-e & RPPO-e & 4766 & 143 & 5091 \\
        DQN-e & A2C-e & 4626 & 99 & 5275 \\
        DQN-e & PPO-e & 4660 & 110 & 5230 \\
        DQN-e & RPPO-e & 4293 & \textbf{145} & \textbf{5562} \\
        PPO-e & A2C-e & 4860 & 98 & 5042 \\
        PPO-e & DQN-e & 5272 & 121 & 4607 \\
        PPO-e & RPPO-e & 4774 & 119 & 5107 \\
        RPPO-e & A2C-e & 5187 & 123 & 4690 \\
        RPPO-e & DQN-e & \textbf{5526} & 116 & 4358 \\
        RPPO-e & PPO-e & 5171 & 91 & 4738 \\
        \hline
    \end{tabular}
\end{table}

Na configuração em que os agentes treinaram, como observamos na Tabela \ref{tab:old_meta_game_aware}, os resultados foram diferentes comparativamente com a configuração determinística. RPPO e A2C ocuparam a primeira e segunda posição, respectivamente, em relação a sua taxa de vitórias. Apesar de terem falhado em generalizar para casos de vitória garantida, no ambiente em que foram todos treinados igualmente, esses dois se desempenharam melhor. É possível que RPPO, devido ao uso de LSTM em seu funcionamento, tenha se adaptado melhor para casos adversos ao invés de garantidamente vantajosos, assim como o algoritmo de \textit{actor-critic} não pudesse avaliar tão bem um estado de muita vantagem, mas pôde obter maior êxito em combates ``semelhantes'' aos vistos durante o treinamento.

Ressalta-se também que, o maior vencedor, RPPO, obteve o maior número de empates, que mesmo sem a informação se foi um empate ``bom'' (a melhor ação realmente seria trocar de Pokémon, o oponente acompanhou e houve empate após muitas trocas) ou ``ruim'' (o agente foi incapaz de causar dano repetidamente pois não aprendeu ainda a generalizar), devido a ele ter maior estatística em ambos os casos, parecem ter sido ``bons'' empates.

\begin{table}[ht]
    \centering
    \caption{Resultados dos jogos com agentes novos contra agentes antigos, no ambiente determinístico. ``V'', ``E'' e ``D'' são o número de vitórias, empates e derrotas que o jogador 1 obteve, respectivamente.}
    \label{tab:versus_full_deterministic}
    \begin{tabular}{llrrr}
        \hline
        Jogador 1 & Jogador 2 & V & E & D \\
        \hline
        A2C-s & A2C-e & 7504 & \textbf{0} & 2496 \\
        A2C-e & A2C-s & \textbf{10000} & \textbf{0} & 0 \\
        DQN-s & DQN-e & 0 & \textbf{0} & \textbf{10000} \\
        DQN-e & DQN-s & \textbf{10000} & \textbf{0} & 0 \\
        PPO-s & PPO-e & \textbf{10000} & \textbf{0} & 0 \\
        PPO-e & PPO-s & 7437 & \textbf{0} & 2563 \\
        RPPO-s & RPPO-e & \textbf{10000} & \textbf{0} & 0 \\
        RPPO-e & RPPO-s & \textbf{10000} & \textbf{0} & 0 \\
        \hline
    \end{tabular}
\end{table}

Como enxergamos na Tabela \ref{tab:versus_full_deterministic}, para cada algoritmo, com exceção do PPO, todos os agentes que treinaram com a recompensa elaborada obtiveram desempenho melhor ou tão bom quanto os agentes novos. No âmbito geral, talvez a recompensa simplificada tenha sido incapaz de, num mesmo número de passos treinados, fazer com que os agentes se aproveitassem do cenário vantajoso. Ainda assim, a observação permanece um pouco inconclusiva, visto que o cenário de vantagem força um tipo específico, então outros cenários de vantagem poderiam ter resultados diferentes.

\begin{table}[ht]
    \centering
    \caption{Resultados dos jogos com agentes novos contra agentes antigos, no ambiente \textit{meta aware}. ``V'', ``E'' e ``D'' são o número de vitórias, empates e derrotas que o jogador 1 obteve, respectivamente.}
    \label{tab:versus_meta_game_aware}
    \begin{tabular}{llrrr}
        \hline
        Jogador 1 & Jogador 2 & V & E & D \\
        \hline
        A2C-s & A2C-e & 6796 & 863 & 2341 \\
        A2C-e & A2C-s & 2387 & 841 & 6772 \\
        DQN-s & DQN-e & 0 & \textbf{880} & \textbf{9120} \\
        DQN-e & DQN-s & \textbf{9184} & 815 & 1 \\
        PPO-s & PPO-e & 4182 & 233 & 5585 \\
        PPO-e & PPO-s & 5494 & 236 & 4270 \\
        RPPO-s & RPPO-e & 4738 & 364 & 4898 \\
        RPPO-e & RPPO-s & 4781 & 366 & 4853 \\
        \hline
    \end{tabular}
\end{table}

Observando a Tabela \ref{tab:versus_meta_game_aware}, vemos que o agente A2C com recompensa simples teve um desempenho melhor que o antigo. No entanto, para todos os outros, a versão da função de recompensa encontrada por engenharia de recompensa parece ter tido desempenho melhor ou semelhante, sendo este último o caso do RPPO, inconclusivo. Forçando uma generalização, a engenharia de recompensa funcionou e, para a maioria dos casos, treinar com esse \textit{feedback} mais elaborado, enxergando um pouco a curto prazo, fez sentido e tornou os agentes mais aptos a batalharem. No entanto, não é verdade que existe uma função de recompensa absoluta, que para todo algoritmo treinado utilizando-a ele terá um desempenho melhor, visto o comportamento do ator-crítico. A própria função de recompensa pode ser vista como um hiperparâmetro que pode ser elaborado para cada agente individualmente.

\begin{table}[ht]
    \centering
    \caption{Resultados dos jogos com a função de recompensa nova, no ambiente determinístico. ``V'', ``E'' e ``D'' são o número de vitórias, empates e derrotas que o jogador 1 obteve, respectivamente.}
    \label{tab:new_full_deterministic}
    \begin{tabular}{llrrr}
        \hline
        Jogador 1 & Jogador 2 & V & E & D \\
        \hline
        A2C-s & DQN-s & 0 & \textbf{10000} & 0 \\
        A2C-s & PPO-s & \textbf{10000} & 0 & 0 \\
        A2C-s & RPPO-s & \textbf{10000} & 0 & 0 \\
        DQN-s & A2C-s & 0 & \textbf{10000} & 0 \\
        DQN-s & PPO-s & \textbf{10000} & 0 & 0 \\
        DQN-s & RPPO-s & \textbf{10000} & 0 & 0 \\
        PPO-s & A2C-s & 0 & \textbf{10000} & 0 \\
        PPO-s & DQN-s & 0 & \textbf{10000} & 0 \\
        PPO-s & RPPO-s & \textbf{10000} & 0 & 0 \\
        RPPO-s & A2C-s & 0 & 0 & \textbf{10000} \\
        RPPO-s & DQN-s & 0 & 0 & \textbf{10000} \\
        RPPO-s & PPO-s & 6164 & 0 & 3836 \\
        \hline
    \end{tabular}
\end{table}


Como evidenciado na Tabela \ref{tab:new_full_deterministic}, ocorreram os primeiros cenários de empate em configurações determinísticas. Com exceção do RPPO, todos, ou como jogador um ou como jogador dois, empataram, e quando o fizeram, foi 100\% dos jogos. O desempenho geral também piorou, pois vemos que o RPPO, por exemplo, mesmo com a vitória garantida na posição de jogador um, perdeu 100\% das vezes na maioria dos casos. Para o ambiente na configuração vantajosa, nenhum agente foi capaz de garantir a sua vitória absoluta, o que pode indicar que não houve generalização para esse cenário a partir da função de recompensa simples.

\begin{table}[ht]
    \centering
    \caption{Resultados dos jogos com a função de recompensa nova, no ambiente \textit{meta aware}. ``V'', ``E'' e ``D'' são o número de vitórias, empates e derrotas que o jogador 1 obteve, respectivamente.}
    \label{tab:new_meta_game_aware}
    \begin{tabular}{llrrr}
        \hline
        Jogador 1 & Jogador 2 & V & E & D \\
        \hline
        A2C-s & DQN-s & 119 & \textbf{4791} & 5090 \\
        A2C-s & PPO-s & 5720 & 1049 & 3231 \\
        A2C-s & RPPO-s & 6163 & 885 & 2952 \\
        DQN-s & A2C-s & 5207 & 4667 & 126 \\
        DQN-s & PPO-s & 7689 & 2211 & 100 \\
        DQN-s & RPPO-s & \textbf{8203} & 1708 & 89 \\
        PPO-s & A2C-s & 3114 & 1087 & 5799 \\
        PPO-s & DQN-s & 120 & 2096 & 7784 \\
        PPO-s & RPPO-s & 5029 & 387 & 4584 \\
        RPPO-s & A2C-s & 2999 & 890 & 6111 \\
        RPPO-s & DQN-s & 101 & 1696 & \textbf{8203} \\
        RPPO-s & PPO-s & 4581 & 390 & 5029 \\
        \hline
    \end{tabular}
\end{table}

Diferente dos melhores agentes observados na Tabela \ref{tab:old_meta_game_aware}, a Tabela \ref{tab:new_meta_game_aware} demonstra resultados diferentes. DQN e A2C obtiveram a maior taxa de vitórias, respectivamente, enquanto PPO e RPPO tiveram desempenho semelhante, sendo o primeiro marginalmente melhor. Uma função de recompensa diferente resultou, para um treino de mesmo número de passos, hiperparâmetros idênticos e com os mesmos agentes avaliados, resultados qualitativos diferentes, o que pode ser evidência que a função de recompensa pode ser vista como um hiperparâmetro capaz de fazermos um \textit{fine-tuning} para cada agente.

\section{Conclusão}

% Após todo o exposto nesta pesquisa, percebemos que uma engenharia de recompensa parece fazer-se necessária. Uma função de recompensa simples demais falha em capturar toda a complexidade de uma batalha Pokémon, mesmo que em um cenário simplificado comparado ao jogo original. No entanto, tal função deve ser pensada com cuidado para não dar oportunidade do agente divergir para um comportamento incorreto durante o seu treinamento, ou ficar preso em decisões a curto prazo, sendo incapaz de levar a batalha até o seu fim.

Após todo o exposto nesta pesquisa, validamos o ambiente de batalha Pokémon simplificado. Através da visualização dos resultados obtidos dos testes entre os agentes treinados, conseguimos observar como eles se moldaram nos cenários de batalha, levando a possíveis melhorias no treino dos agentes e futuro desenvolvimento do ambiente, se aproximando, dessa forma, de sua versão mais completa. Também observamos como uma recompensa simples não foi suficiente para permitir que os modelos capturassem todas as nuances presentes na batalha Pokémon. Dessa forma, disponibilizamos a quem interessar a versão atual do ambiente, que já permite o fácil treinamento de agentes, bem como buscamos continuar evoluindo a pesquisa em prol de reintroduzir \emph{features} removidas e aprimorar a função de recompensa, objetivando fornecer um ambiente completo para treinamento de agentes para a batalha Pokémon.

\subsection{Limitações}
Existem também alguns limitadores. O tempo destinado para a pesquisa estava dentro de um semestre letivo, que foi só parcialmente utilizado para o desenvolvimento desta pesquisa. Como estamos lidando com agentes de aprendizado por reforço, esse tempo fica ainda mais restrito, que para todo agente a ser treinado, para cada configuração diferente de hiperparâmetros, há um tempo razoável de espera para a coleta de resultados. Também houve situações inesperadas, como a atualização de uma biblioteca tornar um código, ainda que comentado, inutilizável, ocasionando na tentativa prolongada de uma refatoração e consequente decisão de descarte de boa parte do trabalho que poderia, na situação ideal, ser reusado.

% Para desenvolvimento futuro, surgem diversas oportunidades. O ambiente simplificado ainda tem um longo caminho para se assemelhar ao jogo original, que possui um estado observável muito maior a ser levado em consideração. Limitação de uso de um mesmo movimento, adição de mais quatro Pokémon na equipe de cada jogador, movimentos que causam efeitos negativos, atributos, itens, entre outros, entram todos nessa categoria. Há a possibilidade do uso de ferramentas de explicabilidade, ganhando entendimento de quais \textit{features} do estado da batalha são as mais relevantes para a tomada de decisão de cada agente. Foram utilizados somente quatro algoritmos para treinar os agentes, existem muitos mais que podem ser testados no ambiente para gerar mais dados de conflitos. Isso e muito mais pode ser feito como trabalho futuro, agregando na evolução da pesquisa.

\subsection{Trabalhos Futuros}
Para desenvolvimento futuro, surgem diversas oportunidades. O ambiente simplificado ainda tem um longo caminho para se assemelhar ao jogo original, que possui um estado observável muito maior a ser levado em consideração. Limitação de uso de um mesmo movimento, adição de mais quatro Pokémon na equipe de cada jogador, movimentos que causam efeitos negativos, atributos, itens, entre outros, entram todos nessa categoria. Há, também, a possibilidade do uso de ferramentas de explicabilidade, ganhando entendimento de quais \textit{features} do estado da batalha são as mais relevantes para a tomada de decisão de cada agente. Além disso, se vê necessário um melhor trabalho de engenharia de recompensas, a fim de evitar casos de recompensa positiva em derrotas, mas ainda permitindo uma maior captura das nuances da batalha, como vista na configuração elaborada.

%Bibliography
\bibliographystyle{plainnat}
\bibliography{references}  


\end{document}
