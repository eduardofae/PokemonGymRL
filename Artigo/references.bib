@misc{poke-env,
    author       = {Haris Sahovic},
    title        = {Poke-env: pokemon AI in python},
    url          = {https://github.com/hsahovic/poke-env}
}

@article{wpl,
   abstract = {Several multiagent reinforcement learning (MARL) algorithms have been proposed to optimize agents decisions. Due to the complexity of the problem, the majority of the previously developed MARL algorithms assumed agents either had some knowledge of the underlying game (such as Nash equilibria) and/or observed other agents actions and the rewards they received. We introduce a new MARL algorithm called the Weighted Policy Learner (WPL), which allows agents to reach a Nash Equilibrium (NE) in benchmark 2-player-2-action games with minimum knowledge. Using WPL, the only feedback an agent needs is its own local reward (the agent does not observe other agents actions or rewards). Furthermore, WPL does not assume that agents know the underlying game or the corresponding Nash Equilibrium a priori. We experimentally show that our algorithm converges in benchmark two-player-two-action games. We also show that our algorithm converges in the challenging Shapleys game where previous MARL algorithms failed to converge without knowing the underlying game or the NE. Furthermore, we show that WPL outperforms the state-of-the-art algorithms in a more realistic setting of 100 agents interacting and learning concurrently. An important aspect of understanding the behavior of a MARL algorithm is analyzing the dynamics of the algorithm: how the policies of multiple learning agents evolve over time as agents interact with one another. Such an analysis not only verifies whether agents using a given MARL algorithm will eventually converge, but also reveals the behavior of the MARL algorithm prior to convergence. We analyze our algorithm in two-player-two-action games and show that symbolically proving WPLs convergence is difficult, because of the non-linear nature of WPLs dynamics, unlike previous MARL algorithms that had either linear or piece-wise-linear dynamics. Instead, we numerically solve WPLs dynamics differential equations and compare the solution to the dynamics of previous MARL algorithms.},
   author = {Sherief Abdallah and Victor Lesser},
   doi = {10.1613/jair.2628},
   journal = {Journal Of Artificial Intelligence Research},
   month = {1},
   pages = {521-549},
   publisher = {AI Access Foundation},
   title = {A Multiagent Reinforcement Learning Algorithm with Non-linear Dynamics},
   volume = {33},
   url = {http://arxiv.org/abs/1401.3454 http://dx.doi.org/10.1613/jair.2628},
   year = {2014}
}
@article{gigawolf,
   abstract = {Learning in a multiagent system is a challenging problem due to two key factors. First, if other agents are simultaneously learning then the environment is no longer stationary, thus undermining convergence guarantees. Second, learning is often susceptible to deception, where the other agents may be able to exploit a learner's particular dynamics. In the worst case, this could result in poorer performance than if the agent was not learning at all. These challenges are identifiable in the two most common evaluation criteria for multiagent learning algorithms: convergence and regret. Algorithms focusing on convergence or regret in isolation are numerous. In this paper, we seek to address both criteria in a single algorithm by introducing GIGA-WoLF, a learning algorithm for normal-form games. We prove the algorithm guarantees at most zero average regret, while demonstrating the algorithm converges in many situations of self-play. We prove convergence in a limited setting and give empirical results in a wider variety of situations. These results also suggest a third new learning criterion combining convergence and regret, which we call negative non-convergence regret (NNR).},
   author = {Michael Bowling},
   journal = {Advances in Neural Information Processing Systems},
   title = {Convergence and No-Regret in Multiagent Learning},
   volume = {17},
   year = {2004}
}
@article{vgc-ai,
   abstract = {This work presents a framework for a new type of meta-game balance AI Competition based on Pokémon, Pokémon battles can be viewed as adversarial games played by AIs. Around these games, there is also a meta-game: which Pokémon to include in a team for battles, which moves to pick for every Pokémon in the team, etc. This meta-game is itself a game with a set of rules that govern which Pokémon and which moves are available in the roster that can be selected from, or which attributes (health points, damage, etc.) a Pokémon or moves should have. The aim of the framework is to facilitate competitions in creating the most balanced meta-game possible; one where there is a large variety of Pokémon and moves to choose from, and many possible combinations that are effective. AI agents could assist human designers in achieving strategically expressive meta-games, and this type of benchmark could incentivize game designers and researchers alike to advance knowledge on this type of domain.},
   author = {Simao Reis and Luis Paulo Reis and Nuno Lau},
   doi = {10.1109/COG52621.2021.9618985},
   isbn = {9781665438865},
   issn = {23254289},
   journal = {IEEE Conference on Computatonal Intelligence and Games, CIG},
   keywords = {AI Competition,Automatic Game Design,Competitive Games,Multi-Agent Systems,Multi-Task Learning},
   publisher = {IEEE Computer Society},
   title = {VGC AI Competition - A New Model of Meta-Game Balance AI Competition},
   volume = {2021-August},
   year = {2021}
}
@article{poke-battle,
   abstract = {Pokémon is one of the most popular video games in the world, and recent interest has appeared in Pokémon battling as a testbed for AI challenges. This is due to Pokémon battling showing interesting properties which contrast with current AI challenges over other video games. To this end, we implement a Pokémon Battle Environment, which preserves many of the core elements of Pokémon battling, and allows researchers to test isolated learning objectives. Our approach focuses on type advantage in Pokémon battles and on the advantages of delayed rewards through switching, which is considered core strategies for any Pokémon battle. As a competitive multi-agent environment, it has a partially-observable, high-dimensional, and continuous state-space, adheres to the Gym de facto standard reinforcement learning interface, and is performance-oriented, achieving thousands of interactions per second in commodity hardware. We determine whether deep competitive reinforcement learning algorithms, WPLθ and GIGAθ, can learn successful policies in this environment. Both converge to rational and effective strategies, and GIGAθ shows faster convergence, obtaining a 100% win-rate in a disadvantageous test scenario.},
   author = {David Simões and Simão Reis and Nuno Lau and Luís Paulo Reis},
   isbn = {9781728170787},
   journal = {2020 IEEE International Conference on Autonomous Robot Systems and Competitions (ICARSC)},
   keywords = {Com-petitive Games,Index Terms-Deep Learning,Multi-Agent Systems,Reinforcement Learning},
   month = {4},
   title = {Competitive Deep Reinforcement Learning over a Pokémon Battling Simulator},
   year = {2020}
}

@article{pettingzoo,
  title={Pettingzoo: Gym for multi-agent reinforcement learning},
  author={Terry, J and Black, Benjamin and Grammel, Nathaniel and Jayakumar, Mario and Hari, Ananth and Sullivan, Ryan and Santos, Luis S and Dieffendahl, Clemens and Horsch, Caroline and Perez-Vicente, Rodrigo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={15032--15043},
  year={2021}
}

@article{lstm,
   abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
   author = {Sepp Hochreiter and Jürgen Schmidhuber},
   doi = {10.1162/NECO.1997.9.8.1735},
   issn = {08997667},
   issue = {8},
   journal = {Neural Computation},
   month = {11},
   pages = {1735-1780},
   pmid = {9377276},
   publisher = {MIT Press Journals},
   title = {Long Short-Term Memory},
   volume = {9},
   url = {https://ieeexplore.ieee.org/abstract/document/6795963},
   year = {1997}
}

@article{dqn,
   abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
   author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
   month = {12},
   title = {Playing Atari with Deep Reinforcement Learning},
   url = {https://arxiv.org/pdf/1312.5602},
   year = {2013}
}

@article{a3c,
   abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
   author = {Volodymyr Mnih and Adria Puigdomenech Badia and Lehdi Mirza and Alex Graves and Tim Harley and Timothy P. Lillicrap and David Silver and Koray Kavukcuoglu},
   isbn = {9781510829008},
   journal = {33rd International Conference on Machine Learning, ICML 2016},
   month = {2},
   pages = {2850-2869},
   publisher = {International Machine Learning Society (IMLS)},
   title = {Asynchronous Methods for Deep Reinforcement Learning},
   volume = {4},
   url = {https://arxiv.org/pdf/1602.01783},
   year = {2016}
}

@article{ppo,
   abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
   author = {John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov Openai},
   month = {7},
   title = {Proximal Policy Optimization Algorithms},
   url = {https://arxiv.org/pdf/1707.06347},
   year = {2017}
}

@article{sb3,
  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},
  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {268},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-1364.html}
}

@article{machinesthink,
   author = {A M Turing},
   journal = {Computing Machinery and Intelligence. Mind},
   pages = {433-460},
   title = {COMPUTING MACHINERY AND INTELLIGENCE},
   volume = {49},
   year = {1950}
}

@article{machinelearning,
   author = {A. L. Samuel},
   doi = {10.1147/RD.33.0210},
   issn = {0018-8646},
   issue = {3},
   journal = {IBM Journal of Research and Development},
   month = {7},
   note = {Termo machine learning é cunhado, algoritmos clássicos em damas},
   pages = {210-229},
   title = {Some Studies in Machine Learning Using the Game of Checkers},
   volume = {3},
   url = {https://ieeexplore.ieee.org/document/5392560},
   year = {1959}
}

